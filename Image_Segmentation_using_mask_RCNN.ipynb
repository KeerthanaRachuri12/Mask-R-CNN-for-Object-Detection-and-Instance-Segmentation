{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeerthanaRachuri12/Mask-R-CNN-for-Object-Detection-and-Instance-Segmentation/blob/main/Image_Segmentation_using_mask_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqrpfEbeBUmg"
      },
      "source": [
        "Data Preparation and Input Modalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "collapsed": true,
        "id": "3RbAC5bX2EC1",
        "outputId": "10aff666-cf85-492c-99e6-d645436ef3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9df0544a-1bb3-46f1-93c5-d72f042c887d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9df0544a-1bb3-46f1-93c5-d72f042c887d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset\n",
            "License(s): unknown\n",
            "Downloading pascal-voc-2012-dataset.zip to /content\n",
            "100% 3.51G/3.52G [00:37<00:00, 186MB/s]\n",
            "100% 3.52G/3.52G [00:37<00:00, 99.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "\n",
        "# Make directory for Kaggle configuration\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Upload kaggle.json manually via Colab file upload and move it to the required directory\n",
        "from google.colab import files\n",
        "files.upload()  # Upload your kaggle.json here\n",
        "\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the NYU Depth V2 dataset\n",
        "!kaggle datasets download -d gopalbhattrai/pascal-voc-2012-dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q pascal-voc-2012-dataset.zip"
      ],
      "metadata": {
        "id": "CspN0_FAVePq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "voc dataset.py"
      ],
      "metadata": {
        "id": "3zu-4iI4zxOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZxgVraGs4PRt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "\n",
        "# Dataset class for Pascal VOC downloaded and uploaded to Colab\n",
        "class VOCDatasetFromFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_folder, image_set_file, transforms=None):\n",
        "        self.root = Path(root_folder)\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Read image IDs list from train/val text files\n",
        "        with open(image_set_file) as f:\n",
        "            self.image_ids = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        self.img_folder = self.root / \"JPEGImages\"\n",
        "        self.anno_folder = self.root / \"Annotations\"\n",
        "        self.class_names = ['__background__','aeroplane','bicycle','bird','boat','bottle',\n",
        "                            'bus','car','cat','chair','cow','diningtable','dog','horse',\n",
        "                            'motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_path = self.img_folder / f\"{img_id}.jpg\"\n",
        "        anno_path = self.anno_folder / f\"{img_id}.xml\"\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        tree = ET.parse(anno_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = int(bbox.find('xmin').text)\n",
        "            ymin = int(bbox.find('ymin').text)\n",
        "            xmax = int(bbox.find('xmax').text)\n",
        "            ymax = int(bbox.find('ymax').text)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "            cls_name = obj.find('name').text\n",
        "            labels.append(self.class_names.index(cls_name))\n",
        "\n",
        "            # Create box mask from bbox\n",
        "            mask = Image.new('1', img.size)\n",
        "            draw = ImageDraw.Draw(mask)\n",
        "            draw.rectangle([xmin, ymin, xmax, ymax], fill=1)\n",
        "            masks.append(torch.as_tensor(np.array(mask), dtype=torch.uint8))\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        masks = torch.stack(masks) if masks else torch.zeros((0, img.height, img.width), dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": image_id,\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "# Simple transform: PIL to Tensor\n",
        "def get_transform():\n",
        "    def transform(img, target):\n",
        "        img = F.to_tensor(img)\n",
        "        return img, target\n",
        "    return transform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "E4GRwu3mXVbi",
        "outputId": "e0c8bc00-7a99-40a3-ca41-e98101cbe80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Collecting torch==2.9.1 (from torchvision)\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->torchvision) (0.7.1)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->torchvision)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch==2.9.1->torchvision)\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n",
            "Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvshmem-cu12\n",
            "    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n",
            "    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n",
            "      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 torchvision-0.24.1 triton-3.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "3dd3d554f4344f18bbe94673d057448f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the model"
      ],
      "metadata": {
        "id": "uq-KorWnb2IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # Load pretrained Mask R-CNN with proper weights argument\n",
        "    weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "    model = maskrcnn_resnet50_fpn(weights=weights)\n",
        "\n",
        "    # Replace the box predictor to match the number of classes (if needed)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # Replace the mask predictor similarly\n",
        "    in_channels = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_channels, hidden_layer, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eWW6NFI2CjBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model"
      ],
      "metadata": {
        "id": "ozUvJTuZbv97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    for images, targets in data_loader:\n",
        "        # Ensure inputs are list of images (each tensor [C,H,W])\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch} complete.\")"
      ],
      "metadata": {
        "id": "hPMUXDklaQDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def main():\n",
        "    # Set paths for your uploaded data in Colab\n",
        "    root_folder = \"/content/VOC2012_train_val/VOC2012_train_val\"\n",
        "    train_txt = \"/content/VOC2012_train_val/VOC2012_train_val/ImageSets/Segmentation/train.txt\"\n",
        "    val_txt = \"/content/VOC2012_train_val/VOC2012_train_val/ImageSets/Segmentation/train.txt\"\n",
        "\n",
        "    num_classes = 21  # 20 classes + background\n",
        "    batch_size = 2\n",
        "    num_epochs = 20  # Reduced training epochs for faster run\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # Prepare datasets and dataloaders\n",
        "    train_dataset = VOCDatasetFromFolder(root_folder, train_txt, transforms=get_transform())\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    model = get_model_instance_segmentation(num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_one_epoch(model, optimizer, train_loader, device, epoch+1)\n",
        "        lr_scheduler.step()\n",
        "        print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "r3yc5aDEA92n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb416d34-2863-498c-ec8b-f0ddf5fbb990",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:00<00:00, 202MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete.\n",
            "Epoch 1 completed.\n",
            "Epoch 2 complete.\n",
            "Epoch 2 completed.\n",
            "Epoch 3 complete.\n",
            "Epoch 3 completed.\n",
            "Epoch 4 complete.\n",
            "Epoch 4 completed.\n",
            "Epoch 5 complete.\n",
            "Epoch 5 completed.\n",
            "Epoch 6 complete.\n",
            "Epoch 6 completed.\n",
            "Epoch 7 complete.\n",
            "Epoch 7 completed.\n",
            "Epoch 8 complete.\n",
            "Epoch 8 completed.\n",
            "Epoch 9 complete.\n",
            "Epoch 9 completed.\n",
            "Epoch 10 complete.\n",
            "Epoch 10 completed.\n",
            "Epoch 11 complete.\n",
            "Epoch 11 completed.\n",
            "Epoch 12 complete.\n",
            "Epoch 12 completed.\n",
            "Epoch 13 complete.\n",
            "Epoch 13 completed.\n",
            "Epoch 14 complete.\n",
            "Epoch 14 completed.\n",
            "Epoch 15 complete.\n",
            "Epoch 15 completed.\n",
            "Epoch 16 complete.\n",
            "Epoch 16 completed.\n",
            "Epoch 17 complete.\n",
            "Epoch 17 completed.\n",
            "Epoch 18 complete.\n",
            "Epoch 18 completed.\n",
            "Epoch 19 complete.\n",
            "Epoch 19 completed.\n",
            "Epoch 20 complete.\n",
            "Epoch 20 completed.\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Assuming 'model' is your trained Mask R-CNN model instance\n",
        "\n",
        "# Specify directory to save weights\n",
        "save_dir = \"./saved_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)  # Create directory if needed\n",
        "\n",
        "# Define full path including filename for saved weights\n",
        "save_path = os.path.join(save_dir, \"maskrcnn_trained.pth\")\n",
        "\n",
        "# Save only the model parameters (state_dict)\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model weights saved to: {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "-K4kd2lu_hv4",
        "outputId": "983e713b-e785-46c7-f123-1c75464238d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-331804152.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Save only the model parameters (state_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model weights saved to: {save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation\n"
      ],
      "metadata": {
        "id": "xXrrSo_NbXQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "# Modify these paths and classes as needed\n",
        "xml_folder = \"/content/VOC2012_train_val/VOC2012_train_val/Annotations\"  # Folder with all XML annotation files (train + val)\n",
        "img_folder = \"/content/VOC2012_train_val/VOC2012_train_val/JPEGImages\"   # Folder with all images\n",
        "val_txt_file = \"/content/VOC2012_train_val/VOC2012_train_val/ImageSets/Segmentation/val.txt\"  # Text file listing val image basenames\n",
        "output_json_path = \"./val_annotations_coco.json\"\n",
        "\n",
        "VOC_CLASSES = [\n",
        "    \"aeroplane\",\"bicycle\",\"bird\",\"boat\",\"bottle\",\n",
        "    \"bus\",\"car\",\"cat\",\"chair\",\"cow\",\n",
        "    \"diningtable\",\"dog\",\"horse\",\"motorbike\",\"person\",\n",
        "    \"pottedplant\",\"sheep\",\"sofa\",\"train\",\"tvmonitor\"\n",
        "]\n",
        "class_name_to_id = {cls_name: i + 1 for i, cls_name in enumerate(VOC_CLASSES)}\n",
        "\n",
        "def mask_to_rle_for_coco_json(mask_np):\n",
        "    from pycocotools import mask as maskUtils\n",
        "    # Ensure mask_np is binary (0 or 1) and of type uint8\n",
        "    mask = mask_np.astype(np.uint8)\n",
        "    # pycocotools expects a Fortran-contiguous array\n",
        "    rle = maskUtils.encode(np.asfortranarray(mask))\n",
        "    # 'counts' needs to be a string for JSON serialization\n",
        "    rle['counts'] = rle['counts'].decode('utf-8')\n",
        "    return rle\n",
        "\n",
        "def xml_to_coco(xml_folder, img_folder, val_image_ids_ordered_list, output_json_path):\n",
        "    images = []\n",
        "    annotations = []\n",
        "    categories = [{'id': i + 1, 'name': name} for i, name in enumerate(VOC_CLASSES)]\n",
        "    info = {\n",
        "        \"description\": \"Pascal VOC 2012 Dataset converted to COCO format for validation\",\n",
        "        \"version\": \"1.0\",\n",
        "        \"year\": 2012,\n",
        "        \"contributor\": \"\",\n",
        "        \"date_created\": \"\"\n",
        "    }\n",
        "\n",
        "    ann_id = 1\n",
        "    # Iterate over the provided ordered list of validation image IDs\n",
        "    for image_idx, image_id_str in enumerate(val_image_ids_ordered_list): # image_idx will be 0, 1, 2...\n",
        "        xml_file_name = f\"{image_id_str}.xml\"\n",
        "        xml_path = os.path.join(xml_folder, xml_file_name)\n",
        "\n",
        "        # Ensure the XML file exists before parsing\n",
        "        if not os.path.exists(xml_path):\n",
        "            print(f\"Warning: XML file not found for {image_id_str}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        filename = root.find('filename').text # Original filename, e.g., '2007_000032.jpg'\n",
        "        img_path = os.path.join(img_folder, filename)\n",
        "\n",
        "        # Ensure the image file exists before reading size\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"Warning: Image file not found for {filename}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        with Image.open(img_path) as img:\n",
        "            width, height = img.size\n",
        "\n",
        "        images.append({\n",
        "            \"id\": image_idx, # Use the 0-based index as the COCO image_id, consistent with DataLoader\n",
        "            \"file_name\": filename,\n",
        "            \"width\": width,\n",
        "            \"height\": height\n",
        "        })\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            cls_name = obj.find('name').text\n",
        "            if cls_name not in class_name_to_id:\n",
        "                # Optionally handle unknown classes or skip\n",
        "                continue\n",
        "\n",
        "            category_id = class_name_to_id[cls_name]\n",
        "            bndbox = obj.find('bndbox')\n",
        "            xmin = int(float(bndbox.find('xmin').text))\n",
        "            ymin = int(float(bndbox.find('ymin').text))\n",
        "            xmax = int(float(bndbox.find('xmax').text))\n",
        "            ymax = int(float(bndbox.find('ymax').text))\n",
        "            o_width = xmax - xmin\n",
        "            o_height = ymax - ymin\n",
        "\n",
        "            # Create a binary mask from the bounding box for segmentation\n",
        "            bbox_mask = Image.new('1', (width, height)) # Use image's full width and height\n",
        "            draw = ImageDraw.Draw(bbox_mask)\n",
        "            draw.rectangle([xmin, ymin, xmax, ymax], fill=1)\n",
        "            bbox_mask_np = np.array(bbox_mask)\n",
        "\n",
        "            # Convert the mask to RLE format for COCO segmentation\n",
        "            segmentation_rle = mask_to_rle_for_coco_json(bbox_mask_np)\n",
        "\n",
        "            annotations.append({\n",
        "                \"id\": ann_id,\n",
        "                \"image_id\": image_idx,\n",
        "                \"category_id\": category_id,\n",
        "                \"bbox\": [xmin, ymin, o_width, o_height],\n",
        "                \"area\": o_width * o_height,\n",
        "                \"iscrowd\": 0,\n",
        "                \"segmentation\": segmentation_rle # Now it's an RLE dict\n",
        "            })\n",
        "            ann_id += 1\n",
        "\n",
        "    coco_format = {\n",
        "        \"info\": info,\n",
        "        \"images\": images,\n",
        "        \"annotations\": annotations,\n",
        "        \"categories\": categories\n",
        "    }\n",
        "\n",
        "    with open(output_json_path, 'w') as f:\n",
        "        json.dump(coco_format, f, indent=4)\n",
        "    print(f\"COCO JSON annotation saved to {output_json_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Read validation image IDs as an ordered list from the file\n",
        "    with open(val_txt_file, 'r') as f:\n",
        "        val_image_ids_ordered = [line.strip() for line in f.readlines()]\n",
        "    xml_to_coco(xml_folder, img_folder, val_image_ids_ordered, output_json_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR2ZGDumFzfr",
        "outputId": "a14a9cd4-812e-41a6-c82b-576d5fce2059"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO JSON annotation saved to ./val_annotations_coco.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete any existing model or tensors if assigned\n",
        "# del model  # Uncomment if you want to delete a model variable\n",
        "\n",
        "# Run garbage collector to free up unreferenced memory\n",
        "gc.collect()\n",
        "\n",
        "# Clear unused cached memory in PyTorch CUDA allocator\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "2tLBPH-cnlFO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import json\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "# For get_model_instance_segmentation\n",
        "import torchvision\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "# Dataset class for Pascal VOC downloaded and uploaded to Colab\n",
        "class VOCDatasetFromFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_folder, image_set_file, transforms=None):\n",
        "        self.root = Path(root_folder)\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Read image IDs list from train/val text files\n",
        "        with open(image_set_file) as f:\n",
        "            self.image_ids = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        self.img_folder = self.root / \"JPEGImages\"\n",
        "        self.anno_folder = self.root / \"Annotations\"\n",
        "        self.class_names = ['__background__','aeroplane','bicycle','bird','boat','bottle',\n",
        "                            'bus','car','cat','chair','cow','diningtable','dog','horse',\n",
        "                            'motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_path = self.img_folder / f\"{img_id}.jpg\"\n",
        "        anno_path = self.anno_folder / f\"{img_id}.xml\"\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        tree = ET.parse(anno_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = int(bbox.find('xmin').text)\n",
        "            ymin = int(bbox.find('ymin').text)\n",
        "            xmax = int(bbox.find('xmax').text)\n",
        "            ymax = int(bbox.find('ymax').text)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "            cls_name = obj.find('name').text\n",
        "            labels.append(self.class_names.index(cls_name))\n",
        "\n",
        "            # Create box mask from bbox\n",
        "            mask = Image.new('1', img.size)\n",
        "            draw = ImageDraw.Draw(mask)\n",
        "            draw.rectangle([xmin, ymin, xmax, ymax], fill=1)\n",
        "            masks.append(torch.as_tensor(np.array(mask), dtype=torch.uint8))\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        masks = torch.stack(masks) if masks else torch.zeros((0, img.height, img.width), dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": image_id,\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Simple transform: PIL to Tensor\n",
        "def get_transform():\n",
        "    def transform(img, target):\n",
        "        img = F.to_tensor(img)\n",
        "        return img, target\n",
        "    return transform\n",
        "\n",
        "# Definition for get_model_instance_segmentation\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # Load pretrained Mask R-CNN with proper weights argument\n",
        "    weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "    model = maskrcnn_resnet50_fpn(weights=weights)\n",
        "\n",
        "    # Replace the box predictor to match the number of classes (if needed)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # Replace the mask predictor similarly\n",
        "    in_channels = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_channels, hidden_layer, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Definition for collate_fn\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def mask_to_rle(mask):\n",
        "    from pycocotools import mask as maskUtils\n",
        "    mask = mask.squeeze(0).cpu().numpy()\n",
        "    mask = mask.astype(np.uint8)\n",
        "    rle = maskUtils.encode(np.asfortranarray(mask))\n",
        "    rle['counts'] = rle['counts'].decode('utf-8')  # Needed for json\n",
        "    return rle\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader, device, gt_json_path):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for target, output in zip(targets, outputs):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "                boxes = output['boxes'].cpu().numpy()\n",
        "                scores = output['scores'].cpu().numpy()\n",
        "                labels = output['labels'].cpu().numpy()\n",
        "                masks = output['masks']\n",
        "\n",
        "                for box, score, label, mask in zip(boxes, scores, labels, masks):\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    width = x2 - x1\n",
        "                    height = y2 - y1\n",
        "                    result = {\n",
        "                        \"info\": {\n",
        "                          \"description\": \"Pascal VOC converted dataset\",\n",
        "                          \"version\": \"1.0\",\n",
        "                          \"year\": 2025,\n",
        "                          \"contributor\": \"\",\n",
        "                          \"date_created\": \"2025-11-14\"\n",
        "                          },\n",
        "                        \"image_id\": image_id,\n",
        "                        \"category_id\": int(label),  # Ensure matches dataset category IDs\n",
        "                        \"bbox\": [float(x1), float(y1), float(width), float(height)],\n",
        "                        \"score\": float(score),\n",
        "                        \"segmentation\": mask_to_rle(mask)\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "    results_file = \"results.json\"\n",
        "    with open(results_file, \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    coco_gt = COCO(gt_json_path)\n",
        "    coco_dt = coco_gt.loadRes(results_file)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='segm')  # 'bbox' for bounding box mAP\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()  # Prints mAP and other metrics\n",
        "\n",
        "\n",
        "def main():\n",
        "    root_folder = \"/content/VOC2012_train_val/VOC2012_train_val\"\n",
        "    val_txt = \"/content/VOC2012_train_val/VOC2012_train_val/ImageSets/Segmentation/val.txt\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    val_dataset = VOCDatasetFromFolder(root_folder, val_txt, transforms=get_transform())\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    model = get_model_instance_segmentation(num_classes=21)\n",
        "    model.load_state_dict(torch.load(\"/content/maskrcnn_trained.pth\"))\n",
        "    model.to(device)\n",
        "\n",
        "    gt_json_path = \"/content/val_annotations_coco.json\"\n",
        "\n",
        "    evaluate_model(model, val_loader, device, gt_json_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRm4yCeQbWTL",
        "outputId": "917ffb45-8b42-42bb-fe3a-fe46e3d4a9ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=1.53s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=5.06s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=1.70s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTAVT0gQuVL8oH7u71OAMj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}